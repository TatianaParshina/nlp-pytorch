{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation with PyTorch\n",
    "Generate Text based on Shakespeare text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import os\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "flags = Namespace(\n",
    "    train_file='Shakespeare.txt',\n",
    "    seq_size=32,\n",
    "    batch_size=16,\n",
    "    embedding_size=64,\n",
    "    lstm_size=64,\n",
    "    gradients_norm=5,\n",
    "    initial_words=['I', 'am'],\n",
    "    predict_top_k=5,\n",
    "    checkpoint_path='checkpoint',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the raw data\n",
    "Steps:\n",
    "- Input dataset is a text data file\n",
    "- Text should be splitted into word tokens to train a word-based model\n",
    "- Convert word tokens into integer indices. These will be the input to the network\n",
    "- Train a mini-batch each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(train_file, batch_size, seq_size):\n",
    "    with open(train_file, 'r') as f:\n",
    "        text = f.read()\n",
    "    text = text.split()\n",
    "    \n",
    "    word_counts = Counter(text)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {k: w for k,w in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {w: k for k,w in int_to_vocab.items()}\n",
    "    n_vocab = len(int_to_vocab)\n",
    "    \n",
    "    print('Vocabulary size', n_vocab)\n",
    "    \n",
    "    int_text = [vocab_to_int[w] for w in text]\n",
    "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
    "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
    "    \n",
    "    #target data for the network to learn\n",
    "    #the target of each input word will be its consecutive word\n",
    "    #shift the whole input data to the left by one step\n",
    "    out_text = np.zeros_like(in_text)\n",
    "    out_text[:-1] = in_text[1:]\n",
    "    out_text[-1] = in_text[0]\n",
    "    in_text = np.reshape(in_text, (batch_size, -1))\n",
    "    out_text = np.reshape(out_text, (batch_size, -1))\n",
    "    return int_to_vocab, vocab_to_int, n_vocab, in_text, out_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "##generate batches for training\n",
    "def get_batches(in_text, out_text, batch_size, seq_size):\n",
    "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
    "    for i in range(0, num_batches * seq_size, seq_size):\n",
    "        yield in_text[:, i:i+seq_size], out_text[:, i:i+seq_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_vocab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1670,  966, 1671, ...,  414,    7,   49],\n",
       "       [  46,   10,   26, ...,    1,  732,    7],\n",
       "       [ 190,    0,  239, ..., 1187,   10,   29],\n",
       "       ...,\n",
       "       [   4, 3102,   19, ...,   58,  328,    1],\n",
       "       [3305, 3306,  145, ...,    3,   45,  124],\n",
       "       [   1, 3532, 3533, ...,   42, 3731,    5]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 966, 1671,   93, ...,    7,   49,   46],\n",
       "       [  10,   26,    0, ...,  732,    7,  190],\n",
       "       [   0,  239,  554, ...,   10,   29,   56],\n",
       "       ...,\n",
       "       [3102,   19,   49, ...,  328,    1, 3305],\n",
       "       [3306,  145,   23, ...,   45,  124,    1],\n",
       "       [3532, 3533,    0, ..., 3731,    5,  399]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_text[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "- create a subclass of torch.nn.Module\n",
    "- define the necessary layers in __init__ method\n",
    "- implement the forward pass within forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModule(nn.Module):\n",
    "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
    "        super(RNNModule, self).__init__()\n",
    "        self.seq_size = seq_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size,\n",
    "                            lstm_size,\n",
    "                            batch_first=True)\n",
    "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
    "        \n",
    "    #will take an input sequence and the previous states \n",
    "    #and produce the output together with states of the current timestep\n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.dense(output)\n",
    "\n",
    "        return logits, state\n",
    "    \n",
    "    # reset states at the beginning of every epoch\n",
    "    def zero_state(self, batch_size):\n",
    "        return (torch.zeros(1, batch_size, self.lstm_size),\n",
    "                torch.zeros(1, batch_size, self.lstm_size))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a loss function and a training op\n",
    "def get_loss_and_train_op(net, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    return criterion, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "- get training data\n",
    "- create the network\n",
    "- create loss and training op\n",
    "- for each epoch, we will loop through the batches to compute loss values and update network’s parameters\n",
    "- Call the train() method on the network’s instance (it will inform inner mechanism that we are about to train, not execute the training)\n",
    "- Reset all gradients\n",
    "- Compute output, loss value, accuracy, etc\n",
    "- Perform back-propagation\n",
    "- Update the network’s parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
    "    net.eval()\n",
    "\n",
    "    state_h, state_c = net.zero_state(1)\n",
    "\n",
    "    for w in words:\n",
    "        ix = torch.tensor([[vocab_to_int[w]]]).to(torch.int64)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "    \n",
    "    _, top_ix = torch.topk(output[0], k=top_k)\n",
    "    choices = top_ix.tolist()\n",
    "    choice = np.random.choice(choices[0])\n",
    "\n",
    "    words.append(int_to_vocab[choice])\n",
    "    \n",
    "    for _ in range(100):\n",
    "        ix = torch.tensor([[choice]]).to(torch.int64)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "\n",
    "        _, top_ix = torch.topk(output[0], k=top_k)\n",
    "        choices = top_ix.tolist()\n",
    "        choice = np.random.choice(choices[0])\n",
    "        words.append(int_to_vocab[choice])\n",
    "\n",
    "    print(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n",
    "        flags.train_file, flags.batch_size, flags.seq_size)\n",
    "    \n",
    "    net = RNNModule(n_vocab, flags.seq_size,\n",
    "                    flags.embedding_size, flags.lstm_size)  \n",
    "    \n",
    "    criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    if not os.path.exists(flags.checkpoint_path):\n",
    "        os.mkdir(flags.checkpoint_path)\n",
    "    \n",
    "    for e in range(50):\n",
    "        batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
    "        state_h, state_c = net.zero_state(flags.batch_size)\n",
    "        \n",
    "        for x,y in batches:\n",
    "            iteration += 1\n",
    "            # Tell it we are in training mode\n",
    "            net.train()\n",
    "            # Reset all gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            x = torch.tensor(x).to(torch.int64)\n",
    "            y = torch.tensor(y).to(torch.int64)\n",
    "            \n",
    "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
    "            loss = criterion(logits.transpose(1, 2), y)\n",
    "            \n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "            \n",
    "            loss_value = loss.item()\n",
    "\n",
    "            # Perform back-propagation\n",
    "            loss.backward()\n",
    "            \n",
    "            # gradient clipping\n",
    "            _ = torch.nn.utils.clip_grad_norm_(\n",
    "                net.parameters(), flags.gradients_norm)\n",
    "\n",
    "            # Update the network's parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print loss value to console\n",
    "            if iteration % 100 == 0:\n",
    "                print('Epoch: {}/{}'.format(e, 200),\n",
    "                      'Iteration: {}'.format(iteration),\n",
    "                      'Loss: {}'.format(loss_value))            \n",
    "            \n",
    "            # the model generate some text \n",
    "            if iteration % 1000 == 0:\n",
    "                predict(net, flags.initial_words, n_vocab,\n",
    "                        vocab_to_int, int_to_vocab, top_k=5)\n",
    "                torch.save(net.state_dict(),\n",
    "                           'checkpoint/model-{}.pth'.format(iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size 5232\n",
      "Epoch: 2/200 Iteration: 100 Loss: 6.114497184753418\n",
      "Epoch: 5/200 Iteration: 200 Loss: 4.590234756469727\n",
      "Epoch: 7/200 Iteration: 300 Loss: 3.6697957515716553\n",
      "Epoch: 10/200 Iteration: 400 Loss: 3.264127254486084\n",
      "Epoch: 12/200 Iteration: 500 Loss: 2.620922803878784\n",
      "Epoch: 15/200 Iteration: 600 Loss: 2.296342134475708\n",
      "Epoch: 17/200 Iteration: 700 Loss: 2.0858142375946045\n",
      "Epoch: 20/200 Iteration: 800 Loss: 1.9138472080230713\n",
      "Epoch: 23/200 Iteration: 900 Loss: 1.5676251649856567\n",
      "Epoch: 25/200 Iteration: 1000 Loss: 1.297070026397705\n",
      "I am scarcely twins, when an absolute year. This, for that purpose. The Merchant for refusing and to be legally to be a on time the First Part perhaps from the Poet stood to be thought in certain property; thing but an old English paraphrase as a poet. to the demands by its origin, and there can well shown them all I felt, found it on its victims. been acted at Whitehall has vitiated but at that year, they are not then in all been beholden to his \"small The whole matter from what time he is known by a brief part of them as had been founded at least the best dwelling-houses of his having English language also he might well see, with some time before; Richard the same nobleman in which I have heard.\" my limits do what I cannot dwell many callings, he was the plays of Record the name has of the old record for which is brought out he is yours, to be, a fool what he could well enough; actor was buried beneath the best vein The Poet's second will makes a mock \"a clerk One of an action part of Windsor_ are too short. up with much from\n",
      "Epoch: 28/200 Iteration: 1100 Loss: 1.3167297840118408\n",
      "Epoch: 30/200 Iteration: 1200 Loss: 1.042470097541809\n",
      "Epoch: 33/200 Iteration: 1300 Loss: 0.940621018409729\n",
      "Epoch: 35/200 Iteration: 1400 Loss: 0.5832827687263489\n",
      "Epoch: 38/200 Iteration: 1500 Loss: 0.8106067180633545\n",
      "Epoch: 41/200 Iteration: 1600 Loss: 0.42291685938835144\n",
      "Epoch: 43/200 Iteration: 1700 Loss: 0.5145277380943298\n",
      "Epoch: 46/200 Iteration: 1800 Loss: 0.4471173584461212\n",
      "Epoch: 48/200 Iteration: 1900 Loss: 0.4041471779346466\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
